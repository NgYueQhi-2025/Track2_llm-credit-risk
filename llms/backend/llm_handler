def call_llm(
    prompt: str,
    mode: str = "summary",
    temperature: float = 0.0,
    use_cache: bool = True,
    mock: bool = False,
    api_key: str = "AIzaSyA-JWSWhRRJdIis3ujmBt8iDv7ij3MJRDQ",
    max_retries: int = 5
) -> Dict[str, Any]:
    """
    Unified LLM call function with mode support, caching, and exponential backoff.

    Args:
        prompt: The applicant's raw text data.
        mode: The mode of processing, e.g., 'summary'.
        temperature: LLM generation randomness.
        use_cache: If True, uses disk cache.
        mock: If True, returns a mock response for testing.
        api_key: API key for LLM service.
        max_retries: Maximum retry attempts on failure.

    Returns:
        A dictionary matching the LLM_RESPONSE_SCHEMA.
    """

    # --- Handle Mock Mode ---
    if mock:
        print("WARNING: Using MOCK LLM response.")
        risk_factor = min(len(prompt) / 500, 1.0)
        return {
            "summary": f"MOCK SUMMARY ({mode}): The applicant provided {len(prompt)} characters of text.",
            "sentiment_score": round(0.5 - (risk_factor * 0.4), 4),
            "risky_phrase_count": int(risk_factor * 3) + 1,
            "contradiction_flag": 1 if len(prompt) > 800 else 0,
            "credibility_score": round(1.0 - (risk_factor * 0.2), 4)
        }

    # --- Prepare Cache ---
    cache_key = hashlib.sha256((prompt + mode).encode("utf-8")).hexdigest()
    cache = {}
    try:
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            cache = json.load(f)
    except FileNotFoundError:
        pass
    except Exception as e:
        print(f"WARNING: Failed to load cache: {e}")

    if use_cache and cache_key in cache:
        print(f"DEBUG: Loaded response from cache for key {cache_key}")
        return cache[cache_key]

    # --- Prepare API Call ---
    url = f"https://generativelanguage.googleapis.com/v1beta/models/Default Gemini API Key:generateContent?key={api_key}"
    system_instruction = (
        "You are an expert financial risk assessment analyst. "
        "Analyze the provided loan application text and extract "
        "the five structured features defined in LLM_RESPONSE_SCHEMA. "
        "Provide ONLY the JSON object, strictly adhering to the schema."
    )
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "systemInstruction": {"parts": [{"text": system_instruction}]},
        "generationConfig": {
            "responseMimeType": "application/json",
            "responseSchema": LLM_RESPONSE_SCHEMA,
            "temperature": temperature
        }
    }

    # --- API Call with Exponential Backoff ---
    for attempt in range(max_retries):
        try:
            print(f"DEBUG: Calling LLM (Attempt {attempt + 1})...")
            response = requests.post(url, headers={'Content-Type': 'application/json'}, data=json.dumps(payload))
            response.raise_for_status()

            result = response.json()
            candidate = result.get('candidates', [{}])[0]
            part = candidate.get('content', {}).get('parts', [{}])[0]
            if part.get('text'):
                raw_json_text = part['text'].strip()
                extracted_features = json.loads(raw_json_text)

                # Validate all required keys
                for key in LLM_RESPONSE_SCHEMA['required']:
                    if key not in extracted_features:
                        raise ValueError(f"Extracted feature '{key}' is missing from LLM response.")

                # Save to cache
                if use_cache:
                    cache[cache_key] = extracted_features
                    tmp = CACHE_FILE + ".tmp"
                    try:
                        os.makedirs(os.path.dirname(CACHE_FILE) or ".", exist_ok=True)
                        with open(tmp, "w", encoding="utf-8") as f:
                            json.dump(cache, f, ensure_ascii=False, indent=2)
                        os.replace(tmp, CACHE_FILE)
                        print(f"DEBUG: Saved cache to {CACHE_FILE}")
                    except Exception as e:
                        print(f"WARNING: Failed to save cache: {e}")

                return extracted_features
            else:
                raise ValueError(f"LLM returned empty or unparsable response: {result}")

        except requests.exceptions.HTTPError as e:
            print(f"HTTP Error on attempt {attempt + 1}: {e}. Retrying...")
        except (requests.exceptions.RequestException, ValueError, json.JSONDecodeError) as e:
            print(f"Non-HTTP Error on attempt {attempt + 1}: {e}. Retrying...")

        # Exponential backoff with jitter
        if attempt < max_retries - 1:
            wait_time = 2**attempt + np.random.uniform(0, 1)
            print(f"Waiting for {wait_time:.2f} seconds before retry.")
            time.sleep(wait_time)
        else:
            raise Exception("LLM call failed after maximum retries.")

    raise Exception("LLM call failed due to internal logic error.")
